{"meta":{"title":"库洛洛Blog","subtitle":"","description":"kuurorochan javenchen blog 技术博客","author":"Kuuroro Chan","url":"http://kuuroro.com","root":"/"},"pages":[{"title":"","date":"2020-01-25T16:40:42.214Z","updated":"2020-01-25T16:40:42.214Z","comments":false,"path":"categories/index.html","permalink":"http://kuuroro.com/categories/index.html","excerpt":"","text":""},{"title":"","date":"2020-01-25T16:40:23.166Z","updated":"2020-01-25T16:40:23.166Z","comments":false,"path":"tags/index.html","permalink":"http://kuuroro.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"腾讯课堂WebRTC上麦和快直播实践","slug":"腾讯课堂WebRTC上麦和快直播实践","date":"2020-03-08T09:30:07.000Z","updated":"2020-03-15T12:18:29.039Z","comments":true,"path":"2020/03/08/腾讯课堂WebRTC上麦和快直播实践/","link":"","permalink":"http://kuuroro.com/2020/03/08/%E8%85%BE%E8%AE%AF%E8%AF%BE%E5%A0%82WebRTC%E4%B8%8A%E9%BA%A6%E5%92%8C%E5%BF%AB%E7%9B%B4%E6%92%AD%E5%AE%9E%E8%B7%B5/","excerpt":"","text":"腾讯课堂WebRTC上麦和快直播实践背景2020年春节过后，因为疫情不仅大家无法上班，学生也无法走进学校上课。保障“停课不停学”，腾讯课堂迎来几百万同时在线上课学生的同时，在线教育部的研发同事们也迎来了巨大挑战。 在线教育的音视频上课场景，无外乎看重：低延迟、可互动。而这次还多了要支持庞大的同时在线数。 因此在上课高峰来临前，IMWeb音视频前端小分队的最优先两个任务是： 浏览器中直播上课，需要支持 学生举手上麦 的能力，提升上课互动体验。 受限于互动直播WebRTC无法支持几十万同时在线数，需要接入快直播WebRTC 来保障Web端的上课高峰。 浏览器端举手上麦先展示最终效果如下（画面为老师分享桌面，方便同时展示老师和学生操作）： 很简单：老师先发起举手，学生点击举手后等待老师点名，老师选择学生上台发言，最后学生就能和老师通话了。 对于前端而言，可能会认为类似上麦这种需求在APP/客户端实现的比较多，但在浏览器中能否实现会有两个主要疑问： 主是浏览器有权限调用系统设备（麦克风）吗 ？ 调用设备录音后，是如何将音频数据实时发送上行的？ 浏览器 getUserMedia 采集音视频其实目前大部分浏览器都支持调用系统硬件设备（如麦克风、摄像头）来进行音视频的录制。这就需要通过浏览器API navigator.mediaDevices.getUserMedia，来拿到本地媒体流。以下WebRTC SDK相关代码： 其中 参数 constraints 是约束，如只采集麦克风音频或摄像头视频，你可以这么设置： 1234const mediaStreamContrains = &#123; video: true, audio: true&#125;; 又或者你可以进一步对采集做限制音频：采样率、采样大小、音量、是否回音消除、是否自动降噪、是否自动增益 视频：分辨率宽高、帧率、宽高比、前置/后置摄像头等 12345678910111213const mediaStreamContrains = &#123; video: &#123; frameRate: &#123;min: 15&#125;, width: &#123;min: 640, ideal: 1280&#125;, height: &#123;min: 360, ideal: 720&#125;, aspectRatio: 16/9 &#125;, audio: &#123; echoCancellation: true, noiseSuppression: true, autoGainControl: true &#125;&#125;; 当然navigator.mediaDevices.getUserMedia并不保证能一定获取到媒体数据。和APP调用系统权限一样，浏览器器会弹出询问提示窗口来索取权限。 异常情况包含以下几种： AbortError［中止错误］ NotAllowedError［拒绝错误］ NotFoundError［找不到错误］ NotReadableError［无法读取错误］ OverConstrainedError［无法满足要求错误］ SecurityError［安全错误］ TypeError［类型错误］没有设置Constraint 因此可以对不同类型的错误做相应记录，并给予用户提示。 获取流并添加流上行因为学生上麦只需要音频，因此constraints将audio设为true即可，获取流后添加到上行的RTCPeerConnection中。 123456789const constraints = &#123; audio:true &#125;navigator.mediaDevices.getUserMedia(constraints).then(function(mediaStream) &#123; // senderConnection是上行的rtcpeerconnection senderConnection.addStream(mediaStream)&#125;).catch((error)=&gt;&#123; //根据error给予错误提示&#125;) 以上就是麦克风音频流如何上行的代码，但SDK前后前后内部逻辑更复杂些。 你会疑问：平时听说的回音消除，网络差时防抖都不用处理吗？是的没错！因为这些WebRTC浏览器底层做了很好的处理。前端开发者只需关注业务，直接调用即可。 举手上麦的整体业务+SDK调用流程在大房的互动直播中，最多只能支持6名学生同时上麦。在一个班级房间中，绝大部分学生的大部分时间都只是收看的，只有当老师允许举手时，学生才能举手并上台发言。 在获取腾讯云WebRTC签名进房时，还要对”只收看“和”要上台发言“的两部分学生，区分调整权限参数。 123456789101112131415161718192021const WEBRTCRIGHTFLAG = &#123; ALL: 255, // 所有权限，可以上行 WATCH: 171, // 只能下行&#125;; const params = &#123; uint32_sdk_appid: CLOUDSDKAPPID, uint32_term_id: termId, // 判断只是收看，还是需要互动上麦 uint64_want_privilege_map: !this.needGetAllRight ? WEBRTCRIGHTFLAG.WATCH : WEBRTCRIGHTFLAG.ALL, uint32_platform: PLATFORM.WEB,&#125;;const getSigUrl = `/cgi-proxy/txcloud/GetTlsPrivilege?$&#123;serializeUrlParameFromObj(params)&#125;`;keFetch(getSigUrl, &#123; method: 'GET',&#125;).then((data) =&gt; &#123; this.handleSig(data.userSig)&#125;).catch((err) =&gt; &#123;&#125;) 学生在不同阶段时根据当前需要，在获取UserSig时指定privilege_map参数。 最后结合举手业务流程和上麦的SDK调用流程，梳理总结出下面的流程图： 结合最前面的展示效果，可以看到在学生执行上麦逻辑时，浏览器会提示是否允许获取麦克风设备。允许后，学生执行音频上行到互动直播的WebRTC服务，老师下行便能接收到学生音频流。 快直播WebRTC接入为什么是快直播首先需要说明的是，此处说的快直播和原本互动直播，在浏览器端都是使用WebRTC进行音视频实时通讯，只是背后音视频服务的传输链路有区别。 快直播WebRTC：通过流地址，经由腾讯云WebRTC Server,从腾讯云直播平台拉取直播流。并以WebRTC下发流。且信令服务无关房间逻辑。适合单流纯收看的场景。 互动直播WebRTC ：通过签名userSig鉴权进房，包含房间业务逻辑，主播/观众模式，信令服务通知流状态更新变化，支持观众上麦。 直接对比一下： 方案 房间逻辑 拉流方式 流更新 主辅路 支持上行 快直播WebRTC 无：只用流地址 腾讯云直播平台拉流（目前只支持原流，不支持混流） 无：只通知当流增加/移除，需要业务层实现 无，需要拉两路流，且无法保证同步 不支持 互动WebRTC 有：通过签名鉴权进房 进房后自动下发 有，房间内信令通知 有主辅路，且同步 支持 总体上来说，快直播只适合纯观看的场景，而互动直播WebRTC有房间逻辑，支持主辅路，上麦互动等等特性，相比之下更适合现有的在线教育场景。 但它的服务架构更复杂，受限于种种原因，目前互动直播的WebRTC还无法支持几十万同时在线。而降级方案CDN直播，虽然理论上支持无上限同时收看人数，但确做不到低延迟。 而快直播WebRTC可以支持几十万的低延时直播 ，并且还支持向上扩容。面对腾讯课堂迎来的上课高峰，Web端需要紧急接入快直播，并且改造成适合在线教育的场景。 快直播接入开发对于纯观看直播场景，快直播的接入并不复杂。可以参考文档https://cloud.webrtc.qq.com/doc/WebRTCLiveMan.html 向业务获取老师当前的主辅路流，并接受push通知，实时更新拉取。 前端快直播拉流： 1234// teacherStreamUrl为从业务后台拿的直播流地址const teacherStreamUrl='webrtc://keonline.liveplay.qq.com/live/42824_e705d155a676d5a52e34d2f438ae00d8?txSecret=9456af788e04feb6f89de5cfbc17f588&amp;txTime=158234289'TXWebRTCAPI.pullStream(teacherStreamUrl); 监听获取快直播流，并给到播放器播放 12345678910&#x2F;&#x2F; 流添加TXWebRTCAPI.on(&quot;onAddStream&quot;, (event)&#x3D;&gt;&#123; const teacherStream &#x3D; e.stream&#125;)&#x2F;&#x2F; 流移除TXWebRTCAPI.on(&quot;onRemoveStream&quot;, (event)&#x3D;&gt;&#123; const teacherStream &#x3D; e.stream; const errorCode&#x3D;e.code&#125;) 快直播整体流程当然接入并不会这么简单，因为上课场景并不会单路流从头看到尾，在适配改造前，让我们看下快直播的整体流程。红色为web端快直播的相关流程，可以发现快直播兜兜转转还是从云直播平台拉流，那为什么不直接云直播平台拉流播放呢？ 可以发现快直播本质是：（从云直播平台）拉取流，改用WebRTC形式下行到浏览器直播，目的就是为了更低的延时而已。（对比RTMP/FLV/HLS，WebRTC可以实现1秒内延时）。舍弃互动直播房间逻辑，只负责拉流推流，让快直播WebRTC向上扩容成为可能。 快直播播放器改造由于上课场景并不会单路流从头看到尾，老师可能随时切换PPT、视频、摄像头，并且画中画两路流来也会回切换。 而原本的webrtc播放器只针对互动WebRTC设计，流的切换完全由云webrtc信令通知，播放器再做处理，业务层无需关注。 但是快直播的拉流需要业务Push通知，流地址也要从业务CGI获取，而拉流之后的成功和失败则由云的信令通知。 短时间内专门做一个快直播播放器并不现实，只能在原有播放器做增量适配改造。 改造思路：蓝色为新增。 复用原本播放器 UI/Dom，初始化逻辑，以及流的封装封方法等。 原本互动WebRTC 流管理 和SDK相关逻辑保持不变。 新增快直播SDK调用，及快直播流管理：包含拉流后的流增加和流移除、多路流管理的主辅路关系映射等。 播放器原本接口不变，新增拉流和停止拉流的接口及事件抛出。 流管理基于quickStreams对象，管理对象和主辅路的映射关系。 1234567this.quickStreams = &#123; '42824_e705d155a676d5a52e34d2f438ae00d8':&#123; streamUrl:'webrtc://keonline.liveplay.qq.com/live/42824_e705d155a676d5a52e34d2f438ae00d8?txSecret=9456af788e04feb6f89de5cfbc17f588&amp;txTime=158234289',//流地址 type: 'main',//'main或vice，当前大小画面的映射标识' stream: &lt;MediaStream&gt; //miedaStream &#125;,&#125;; 要注意下三点： 第一点：由于播放器接收业务和云的两放信令控制，由于两者下发时机是有差异的，这就无法很难保证当前的流映射关系是正确的。因此要规定两端信令的控制范围： 业务信令只控制流状态映射的更新，而云信令只控制流能否播放 第二点：由于快直播sdk无法二次拉取相同的流。收到更新和拉流前，遍历quickStreams中流是否已经存，并检查流状态。如果存在切正常则无需重新拉流，只更新流的主辅路对大小画面的映射关系。 第三点：由于快直播多路流，通过多个PeerConnection传输（互动webrtc主辅路两路流只通过一个PeerConnection），因此会存在主辅路轻微不同步的问题。这时候音频应该以大画面为准，避免音画不同步。 完善监控与上报这次接入快直播WebRTC，除了补充快直播的监控上报外，还对增强原本的监控逻辑。 removeStream 异常监控1234567case 20004:&#x2F;&#x2F; 防盗链鉴权服务器内部错误，建议重试case 20005:&#x2F;&#x2F; 服务器内部错误，建议重试case 20009:&#x2F;&#x2F; 数据通道连接超时，出现该错误，业务可以提示用户网络异常，刷新重试case 20011:&#x2F;&#x2F; 流异常断开,可能是主播下播或者网络异常，建议重试重新拉流case 20012:&#x2F;&#x2F; 流结束case 20014:&#x2F;&#x2F; 防盗链鉴权失败case 20015:&#x2F;&#x2F; 客户端跟后台数据通道超时断开 黑屏检测：当收到视频帧后，且未下课前。每秒检测黑屏情况，超出阀值做降级处理。检测方法如下：123456setInterval(() &#x3D;&gt; &#123; const mainVideo&#x3D;his.mainVideo.getDom() if(mainVideo.srcObject &amp;&amp; mainVideo.srcObject.active &amp;&amp; mainVideo.readyState !&#x3D;&#x3D; 4)&#123; &#x2F;&#x2F; 判断为黑屏 &#125;&#125;,1000) 加入丢包率与卡顿率上报：SDK调用浏览getStats API获取WebRTC报告数据，对packetLost、packetsReceived、interframeDelay这些核心度量数据做收集。快直播SDK每2s抛出质量数据，但由于web侧几十万同时在线数，频繁上报带来会带较大大压力。因此基于质量数据做60s的前端合并计算后， 再进行上报。 123456789101112131415161718192021222324type: &quot;stream_quality&quot;content:decInfo:streamid: &quot;42824_2b851a8b0e208fa8c5f0c129ed6daf43&quot;streamurl: &quot;webrtc:&#x2F;&#x2F;keonline.liveplay.qq.com&#x2F;live&#x2F;42824_2b851a8b0e208fa8c5f0c129ed6daf43?txSecret&#x3D;3e403590486c92f3f63e7293545c597d&amp;txTime&#x3D;1583733346&quot;audio:ssrc: &quot;16884383&quot;codec: &quot;opus&quot;packetsReceived: 104bytesReceived: 12424packetsLost: 0video:ssrc: &quot;33750382&quot;codec: &quot;H264&quot;packetsReceived: 29bytesReceived: 4279packetsLost: 0width: 1328height: 672framesDecoded: 13framesReceived: 14interFrameDelayMax: 136 总结虽然此次紧急需求在短短几天就完成开发并上线，但由于开发时间短，代码层面没有完全很好梳理： 受限播放器原本的设计，只适用于互动WebRTC，这次采取增量改造方式来开发。互动WebRTC、快直播WebRTC、FLV、HLS直播间的切换都需要播放器层级的销毁与重载，应该还能再优化。 应对高峰课我们只能继续加很多监控点，但上报点分散，包袱越来越多，急需后续对Web端和H5端上报点做统一归拢，根据业务和端做上报的配置化，从而不必频繁多端改造播放器。 在 “快速保障业务优先“ 的前提下，此次暴露了一些没有发现的问题，虽还掉了一些欠债，但原本播放器打补丁式的新增开发，也增添了些新债务。我们没法避免飞快的任务需求节奏，除了开发效率，也要保持所负责的代码时刻处于合理健康的状态，视角也要面向未来潜在的挑战。 小分队讨论接下来做一款设计更合理的，并且支持多种形式播放的Web直播播放器。目标是体验和质量更优、提升维护性和扩展性。 写在最后对于音视频前端小分队而言，以上提到的只是小小一部分。在超哥带领下快速完成包括：协调扩容，流控降级策略，完善监控，日志排查，体验优化和改进，并同各位同事一起遇到问题就解决问题。 随着腾讯课堂PCU不断突破新高，仅Web浏览器端就有近60万的同时在线上课高峰。 最后特别提及 jayccchen 超哥、lizzydeng 邓雪，以及各位OED开发同学和腾讯云同事日以继日加班加点的付出，保证了此次腾讯课堂高峰课web端上课的音视频质量。 PS： 如果有想同我们深入研究、共同探索 web 音视频的同学，欢迎加入我们 IMWeb 团队。在线教育部有许多音视频场景，期待你施展拳脚，感兴趣的同学可以直接联系我。","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2020-01-25T15:58:10.928Z","updated":"2020-01-25T15:58:10.928Z","comments":true,"path":"2020/01/25/hello-world/","link":"","permalink":"http://kuuroro.com/2020/01/25/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"WebRTC前端全攻略","slug":"WebRTC前端全攻略","date":"2019-08-31T04:09:46.000Z","updated":"2020-03-08T04:53:54.904Z","comments":true,"path":"2019/08/31/WebRTC前端全攻略/","link":"","permalink":"http://kuuroro.com/2019/08/31/WebRTC%E5%89%8D%E7%AB%AF%E5%85%A8%E6%94%BB%E7%95%A5/","excerpt":"","text":"一、WebRTC基础介绍什么是webRTCWebRTC 全称是（Web browsers with Real-Time Communications (RTC)大概2011年，谷歌收购了 GIPS，它是一个为 RTC 开发出许多组件的公司，例如编解码和回声消除技术。Google 开源了 GIPS 开发的技术，并希望将其打造为行业标准。 在QQ2004版后，通话质量大有改善，就是采用了GIPS的技术。并第一次表明版权：本软件中使用的GIPS语音引擎和相关商标为Global IP Sound AB公司版权所有~ 收购花了一大笔钱，谷歌说开源就开源，确实不得不佩服，但显然对于Googl来说，打造音视频的开源生态有着更大的价值。“浏览器 + WebRTC”就是 Google 给出的一个答案。而它的终极目标就是在浏览器之间实现音视频通信。 发展至今日，WebRTC在浏览器的支持性已经大大增强。WebRTC是一个免费、开放的项目。使web浏览器通过简单的JavaScript api接口实现实时通信功能。 WebRTC应用场景 在线教育 多人音视频实时通话 网络直播 WebRTC原理与架构 Web API层：面向开发者提供标准API（javascirpt），前端应用通过这一层接入使用WebRTC能力。 C++ API层：面向浏览器开发者，使浏览器制造商能够轻松地实现Web API方案。 音频引擎（VoiceEngine）：音频引擎是一系列音频多媒体处理的框架，包括从视频采集卡到网络传输端等整个解决方案。 iSAC/iLBC/Opus等编解码。 NetEQ语音信号处理。 回声消除和降噪。 视频引擎（VideoEngine）： 是一系列视频处理的整体框架，从摄像头采集视频、视频信息网络传输到视频显示整个完整过程的解决方案。 VP8编解码。 jitter buffer：动态抖动缓冲。 Image enhancements：图像增益。 传输（Transport)：传输 / 会话层，会话协商 + NAT穿透组件。 RTP 实时协议。 P2P传输 STUN+TRUN+ICE实现的网络穿越。 硬件模块：音视频的硬件捕获以及NetWork IO相关。 WebRTC重要的类及概念Network Stream API - MediaStream（媒体流）/MediaStreamTrack（媒体轨）这个类并不完全属于WebRTC的范畴，但是在本地媒体流获取，及远端流传到vedio标签播放都与WebRTC相关。MS 由两部分构成： MediaStreamTrack 和 MediaStream。 MediaStreamTrack 是媒体轨，代表一种单类型数据流，可以是音频轨或者视频轨。 MediaStream 是一个完整的音视频流。它可以包含 &gt;=0 个 MediaStreamTrack。它主要的作用就是确保几个媒体轨道是同步播放。 RTCPeerConnectionWebRTC使用RTCPeerConnection，用于实现peer跟peer之间RTC连接，继而无需服务器就能传输音视频数据流的连接通道。（直播的实际生产中还是需要服务器）。 这么说过于抽象，为了帮助理解，可以用一个不太恰当但有助于理解的比喻：RTCPeerConnection就是一个高级且功能强大的用于传输音视频数据而建立类似Websocket链接通道，只不过它不仅可以client to server还可以 之所以说是高级且强大，是因为它作为WebRTC web层核心API，让你无须关注数据传输延迟抖动、音视频编解码，音画同步等问题。直接使用PeerConnection 就能自动用上这些浏览器提供的底层封装好的能力。 Peer-to-peer Data APIRTCDataChannel可以建立浏览器之间的点对点通讯。常用的通讯方式有websocket, ajax和等方式。websocket虽然是双向通讯，但是无论是websocket还是ajax都是客户端和服务器之间的通讯，你必须配置服务器才可以进行通讯。 而由于RTCDATAChannel借助RTCPeerConnection无需经过服务器，就可以提供点对点之间的通讯，无需/(避免)服务器了这个中间件。 二、浏览器的音视频采集及设备管理音视频采集基本概念在讲浏览器提供的用JS 采集音视频API之前，需要先了解音视频采集的基本概念。 摄像头：用于捕捉（采集）图像和视频。 帧率：Frame rate 摄像头一秒钟采集图像的次数称为帧率。。 分辨率：用于度量图像内数据量多少的一个参数，通常表示成ppi。分辨率越高越清晰，但占用的宽带越多，应当根据网络情况动态调整。 麦克风: 用于采集音频数据。 采样率：指录音设备在一秒钟内对声音信号的采样次数，采样率越高还原声音越真实。比如8000 Hz 是 电话通话所用采样率。 轨（Track）: WebRTC 中的“轨”借鉴了多媒体的概念。“轨”在多媒体中表达的就是每条轨数据都是独立的，分为音频轨、视频轨。 流（Stream）: 可以理解为轨的容器。在 WebRTC 中，“流”可以分为媒体流（MediaStream）和数据流（DataStream）。 音视频设备与采集getUserMediagetUserMedia 方法在浏览器中访问音视频设备非常简单。 1var promise = navigator.mediaDevices.getUserMedia(constraints); 结果会通过Promise返回stream，用URL.createObjectURL/srcObject转换后，设置为Video或Audio元素的src属性来进行播放。 失败时promise catchError 可能的异常有： AbortError：硬件问题 NotFoundError：找不到满足请求参数的媒体类型。 NotReadableError：操作系统上某个硬件、浏览器或者网页层面发生的错误导致设备无法被访问。 TypeError：类型错误，constraints对象未设置空，或者都被设置为false。 OverConstrainedError：指定的要求无法被设备满足。 SecurityError：安全错误，需要用户浏览器设置中开启。 NotAllowedError：用户拒绝了当前的浏览器实例的访问请求；或者用户拒绝了当前会话的访问；或者用户在全局范围内拒绝了所有媒体访问请求。 MediaStreamConstraints 参数媒体约束-MediaStreamConstraints，可以在 getusermedia时指定 MediaStream 中要包含哪些类型的媒体轨，并且设置一些限制。 可以指定采集音频还是视频，或是同时对两者进行采集。 1234const mediaStreamContrains = &#123; video: true, audio: true&#125;; 也可以进一步对媒体做限制。 123456789101112131415161718192021const mediaStreamContrains = &#123; video: &#123; frameRate: &#123; min: 20 &#125;, width: &#123; min: 640, ideal: 1280 &#125;, height: &#123; min: 360, ideal: 720 &#125;, aspectRatio: 16/9 &#125;, audio: &#123; echoCancellation: true, noiseSuppression: true, autoGainControl: true &#125;&#125;; 使用采集到的音视频媒体流通过getUserMedia采集到的媒体流,可以再本地直接播放使用。 1&lt;video autoplay playsinline&gt;&lt;/video&gt; 12345678910111213const mediaStreamContrains =&#123; video: true &#125;;const localVideo = document.querySelector('video');function gotLocalMediaStream(mediaStream) &#123; localVideo.srcObject = mediaStream;&#125;function handleLocalMediaStreamError(error) &#123; console.log('navigator.getUserMedia失败: ', error);&#125;navigator.mediaDevices.getUserMedia(mediaStreamContrains).then(gotLocalMediaStream).catch(handleLocalMediaStreamError); 音视频设备管理MediaDevices接口提供了访问（连接到计算机上的）媒体设备（如摄像头、麦克风）以及屏幕分享的方法。 我们可以通过它，获取可用的音视频设备列表。 MediaDeviceInfo，它表示的是每个输入 / 输出设备的信息: deviceID： 设备的唯一标识 label： 设备名称 kind：设备种类：可用于识别出是音频设备还是视频设备，是输入设备还是输出设备。 需要注意的是，出于安全原因，除非用户已被授予访问媒体设备的权限（要想授予权限需要使用 HTTPS 请求），否则 label 字段始终为空。 另外，label 可以用作指纹识别机制的一部分，以识别是否是合法用户。 获取音视频设备列表1MediaDevices.enumerateDevices().then((deviceList)=&gt;&#123;console.log(deviceList)&#125;) 通过调用navigator.MediaDevices.enumerateDevices()返回每一个 MediaDeviceInfo，并将每个 MediaDeviceInfo 中的基本信息打印出来，也就是我们想要的每个音视频设备的基本信息。 可以通过 kind 字段再将设备区分为：音频/视频设备，输入/输出设备。根据deviceid，能知道该设备是否为默认设备。 这个音频设备为例，将耳机插入电脑后，耳机就变成了音频的默认设备；将耳机拔出后，默认设备又切换成了系统的音频设备。 浏览器端的音视频录制录制从端来说， 可以分为服务端录制和客户端录制。 服务端录制：无须心客户端因电脑问题造成录制失败（如磁盘空间不足、CPU 占用率过高等问题）；缺点是实现的复杂度很高。 客户端录制：优点是方便录制者（如老师）操控，所录制的视频清晰度高,实现相对简单。缺点是，录制时会开启的编码器，很耗CPU，且对内存和硬盘要求也高，一旦硬件占高负载会容易造成程序卡死。 JavaScript的二进制数据对象在 JavaScript 中，有很多用于存储二进制数据的类型，这些类型包括：ArrayBuffer、ArrayBufferView 和 Blob。WebRTC 录制音视频流之后，最终是通过 Blob 对象将数据保存成多媒体文件的。 ArrayBufferArrayBuffer 对象表示通用的、固定长度的二进制数据缓冲区。因此，你可以直接使用它存储图片、视频等内容。 123let buffer = new ArrayBuffer(16); // 创建一个长度为 16 的 bufferlet view = new Uint32Array(buffer); 12let buffer = new Uint8Array([255, 255, 255, 255]).buffer;let dataView = new DataView(buffer); 一开始生成的 buffer 是不能被直接访问的。只有将 buffer 做为参数生成一个具体的类型的新对象时（如 Uint32Array 或 DataView），这个新生成的对象才能被访问。 ArrayBufferView ArrayBufferView 并不是一个具体的类型，而是代表不同类型的 Array 的描述。这些类型包括：Int8Array、Uint8Array、DataView 等。也就是说 Int8Array、Uint8Array 等才是 JavaScript 在内存中真正可以分配的对象。 BlobBlob（Binary Large Object）是 JavaScript 的大型二进制对象类型，WebRTC 最终就是使用它将录制好的音视频流保存成多媒体文件的。而它的底层是由上面所讲的 ArrayBuffer 对象的封装类实现的，即 Int8Array、Uint8Array 等类型。 1var aBlob = new Blob( array, options ); 浏览器录制方法浏览器为我们提供了一个录制音视频的类，即 MediaRecorder。 1var mediaRecorder = new MediaRecorder(stream[,options]); 参数解释： stream: 通过 getUserMedia 获取的本地视频流或通过 RTCPeerConnection 获取的远程视频流。 options:可选项，指定视频格式、编解码器、码率等相关信息，如 mimeType: ‘video/webm;codecs=vp8’。 录制流 1234567891011121314151617181920212223242526272829303132333435363738var buffer;//当该函数被触发后，将数据压入到blob中function handleDataAvailable(e)&#123; if(e &amp;&amp; e.data &amp;&amp; e.data.size &gt; 0)&#123; buffer.push(e.data); &#125;&#125;function startRecord()&#123; buffer = []; //设置录制下来的多媒体格式 var options = &#123; mimeType: 'video/webm;codecs=vp8' &#125; //判断浏览器是否支持录制 if(!MediaRecorder.isTypeSupported(options.mimeType))&#123; console.error(`$&#123;options.mimeType&#125; is not supported!`); return; &#125; try&#123; //创建录制对象 mediaRecorder = new MediaRecorder(window.stream, options); &#125;catch(e)&#123; console.error('Failed to create MediaRecorder:', e); return; &#125; //当有音视频数据来了之后触发该事件 mediaRecorder.ondataavailable = handleDataAvailable; //开始录制 mediaRecorder.start(10);&#125;... 播放录制文件首先根据 buffer 生成 Blob 对象；然后，根据 Blob 对象生成 URL，并通过 video标签进行播放。 1&lt;video id=\"playback\"&gt;&lt;/video&gt; 123456var blob = new Blob(buffer, &#123;type: 'video/webm'&#125;);playback.src = window.URL.createObjectURL(blob);playback.srcObject = null;playback.controls = true;playback.play(); 屏幕分享桌面分享可以当做特殊音视频数据来看待，在实时音视频，尤其是在在线教育场景中，尤为常见。 对于屏幕分享者：每秒钟多次抓取的屏幕，每一屏数据取它们的差值，然后对差值进行压缩；如果差值超过一定程度，则单独对这一屏数据进行帧内压缩，该压缩方法，类似于视频编码中GOP的I帧。 对于远端观看/控制者：收到数据进行解压缩，还原成画面播放即可。另外如果有操控指令，需要实现对应的信令系统，并自行信令控制。 总结为以下流程：抓屏、压缩编码、传输、解码、显示、控制。和音视频流程基本一致。 屏幕分享的协议有： RDP（Remote Desktop Protocal）：windows下的桌面共享协议。 VNC（Virtual Network Console）：在不同的操作系统上共享远程桌面，像 TeamViewer、RealVNC 都是在使用这个协议。 WebRTC的屏幕分享由于webrtc不包含控制部分，因此它的处理过程只使用了视频方式，而不需要信令控制。因此它和 RDP/VNC还是存在差异。 桌面数据的采集在桌面数据采集上，和VNC一样是通过各平台提供的API实现的。BitBlt、Hook、DirectX等。最新的 WebRTC 都是使用的这种方式GetWindowDC：可以通过它来抓取窗口。 桌面数据的编码WebRTC 对桌面的编码使用的是视频编码技术，即 H264/VP8 等（好处就是压缩率高）；但 RDP/VNC 则不一样，它们使用的是图像压缩技术。 传输webrtc有根据网络情况的调节能力，网络差时会进行丢数据保证实时性。 解码渲染解码同第二点，渲染一般会通过 OpenGL/D3D 等 GPU进行渲染。 通过 getDisplayMedia API 来采集桌面： 特别注意: 在桌面采集的参数里，不能对音频进行限制了。也就是说，不能在采集桌面同时采集音频。 123456789101112131415161718//get桌面数据流function getDeskStream(stream)&#123; localStream = stream;&#125;//采集桌面function shareDesktop()&#123; //只有在 PC 下才能抓取桌面 if(utils.isPC)&#123; //开始捕获桌面数据 navigator.mediaDevices.getDisplayMedia(&#123;video: true&#125;) .then(getDeskStream) .catch(handleError); return true; &#125; return false; &#125; 而展示和录制和音视频的展示和录制是实现方式基本相同。 以上就是浏览器端WebRTC通过设备采集音视频数据及其播放与录制的相关介绍。有了数据，接下来才可以使用WebRTC来实现实时音视频通讯。 三、WebRTC的信令服务为什么WebRTC没有信令服务的规范信令服务（Signaling Server）一般指能够提供给各节点交换协商信息和控制信号的服务。对于WebRTC而言，两端在数据传输前，必须交换两端的协商信息（如支持的编解码、所处网络位置等）和以及通信中的控制信号（如哪个房间、何时上下线，是否接收/拒绝等）。 那么WebRTC 规范中是否提供了这样的信令服务标准呢，答案是：没有！ WebRTC 规范中定义许多功能及其API，受益于其一致性，让开发者在客户端（浏览器）能更方便的开发。但是并没有提供信令相关的约束标准，原因在于： 虽然WebRTC的应用场景有相似之处，但每个业务中都存在特殊需求，因此不限制信令服务。 WebRTC更聚焦于处理音视频处理及传输的实现，不约束信令服务其实更有利于向推广各公司业务使用WebRTC。 信令服务在WebRTC中作用WebRTC主要用于音视频通信的场景中，也和一般的IM系统有着相似之处。它们信令服务大多存在”房间”这一业务概念。 房间控制：每个用户通讯前，先进入房间（join），这里可能会涉及到鉴权。当结束通讯时需要离开房间（leave）。亦或者由于某些权限者可以将用户踢出房间（kick）。这些都有业务开发自行决定。 协商信令的交换：在进房后音视频通讯前，需要进行协商信令的交换：比如两端SDP（offer/answer）、ICE Candidate的交换等。 前端视角下的信令服务实现业界有很多已经开源的信令服务，开发者可以基于它们在做再定制。 身为前端开发你可以去找开源的信令服务，如果要自己简单实现，那么Node.js + socket.io的组合不失为一个容易上手的方案。而浏览器中与信令服务通信方式，似乎也没有比WebSocket更适合的了。 简单来说socket.io就是对websocket、轮训机制及其它实时通讯能力的封装，但我们主要还是为了使用其中websocket。并且由于它还内置了 “房间” 逻辑，我们可以并利用其进行 “信令广播“，来做媒体协商信令的下发以及交换等。 四、WebRTC数据传输协议-UDP/RTP/RTCP音视频数据传输：UDP还是TCP？我们都知道http协议，运行于TCP协议之上，是万维网的运转的基础。TCP作为传输层协议，使用三次握手协议建立连接（经典考题），以及一系列措施措施来确保传输的可靠性。那么WebRTC为什么选择UDP而不是TCP呢？ 首先来看TCP和UDP的区别： TCP是面向连接的，UDP是无连接的。 TCP提供可靠的创术，也就是说TCP连接传输的数据不会丢失，没有重复，并且按顺序到达，UDP提供不可靠传输。 TCP是面向字节流的，实际上是TCP把数据看成一连串无结构的字节流；UDP是面向报文的。 TCP提供拥塞控制和流量控制机制； UDP不提供拥塞控制和流量控制机制。 既然TCP有这么多优点特性，那么为什么在音视频传输中使用UDP呢？ 由于UDP不属于连接型协议，因而具有资源消耗小，处理速度快的优点。 通常实时音频视频数据在传输的少量数据包丢失，对接收者影响并不大。而此时TCP的可靠性反而成了累赘，TCP协议的超时重传机制中RTT是以2的指数的增长，7次仍失败将会断开，理论计算会达到2分钟，在音视频延迟中相当差的效果。 这种只管发不管收的特性，使得UDP在实时性和效率性都很高，在实时音视频传输中通常会选用UDP协议作为音视频传输协议 WebRTC也是如此，在信令控制方面采用了可靠的TCP，而在音视频数据传输上采用了UDP,以及基于此的SRTP和SRTCP（加密的RTP和RTCP)。 RTP和RTCP在音视频传输中，并不直接使用UDP，而是需要RTP包装音视频数据上再通过UDP传输。 原因在于，音视频中一个帧数据量需要多个个包来传送，并在接收端组成对应帧。而UDP并没有这个能力，RTP协议就是承载这个功能。 RTP协议 RTP全名Real-timeTransportProtocol(实时传输协议)，主用于对媒体数据的封装，用于实时传输数据。RTP报文由两部分组成：报头和有效载荷。 以下为RTP协议头的解释，前12字节是固定的，CSRC可以有多个或者0个。 V：RTP协议的版本号，占2位，当前协议版本号为2 P：填充标志，占1位，如果P=1，则在该报文的尾部填充一个或多个额外的八位组，它们不是有效载荷的一部分 X：扩展标志，占1位，如果X=1，则在RTP报头后跟有一个扩展报头 CC：CSRC计数器，占4位，指示CSRC标识符个数 M：标志，占1位，不同的有效载荷有不同的含义，对于视频，标记一帧的结束；对于音频，标记会话的开始。 PT（payload type）：有效荷载类型，占7位，用于记录RTP报文中有效载荷的类型/Codec，在流媒体中大部分是用来区分音频流和视频流，便于接收端（receiver）找出相应的 decoder 解碼出來。 序列号（sequence number）：占16位，用于标识发送者所发送的RTP报文的序列号，每发送一个报文，序列号增1。这个字段当下层的承载协议用UDP的时候，网络状况不好的时候可以用来检查丢包。当出现网络抖动的情况可以用来对数据进行重新排序。序列号的初始值是随机的，同时音频包和视频包的sequence是分别计数的。 时戳（timestamp）：占32位，必须使用90kHZ时钟频率（程序中的90000）。时戳反映了该RTP报文的第一个八位组的采样时刻。接受者使用时戳来计算延迟和延迟抖动，并进行同步控制。可以根据RTP包的时间戳来获得数据包的时序。 同步信源（SSRC）标识符：占32位，用于标识同步信源。同步信源是指产生媒体流的信源，它通过RTP报头中的一个32为数字SSRC标识符来标识，而不依赖网络地址，接收者将根据SSRC标识符来区分不同的信源，进行RTP报文的分组。 提供信源（CSRC）标识符：每个CSRC标识符占32位，可以有0~15个CSRC。每个CSRC标识了包含在RTP报文有效载荷中的所有提供信源。 RTCP协议在传输音视频时的丢包，乱序，抖动，这些WebRTC在底层都有对应的处理策略。但是如何将这些传输时 “网络质量信息” 实时告诉对方，就是RTCP所承载的作用。 RTP报文有多种类型： 发送报告SR (Sender Report)：当前活动发送者发送、接收统计。PT=200 接受者报告RR (Reciver Report)：接收报告，非活动发送者接收统计。PT=201 源描述报告SDES(Source Description)：源描述项，包括CNAME PT=202 BYB报告：参与者结束对话 PT=203 APP报告：应用自定义 PT=204 jitter报告IJ PT=195 传输反馈RTPFB PT=205 Playload反馈PSFB PT=206 . . . . . 这里我们可以关注两个比较重要的的报文：SR 和 RR,通过它们让收发两端知道网络质量情况。 以上为SR的协议规范： Header 部分用于标识该报文的类型，比如是 SR（200） 还是 RR（201）。 Sender info 部分用于指明作为发送方，到底发了多少包。 Report block 部分指明发送方作为接收方时，它从各个 SSRC 接收包的情况。 通过报告以上信息，各端知道网络质量后，就可以根据其做传输策略的调整了。 RTP会话流程当了解为什么选择UDP协议、以及RTP/RTCP协议做了些什么事情之后，让简单总结在传输协议层面上的整个流程： 当应用建立一个RTP会话时，应用程序将确定一对目的传输地址。目的传输地址由一个网络地址和一对端口组成，有两个端口：一个给RTP包，一个给RTCP包。RTP数据发向偶数的UDP端口，而对应的控制信号RTCP数据发向相邻的奇数UDP端口（偶数的UDP端口＋1），这样就构成一个UDP端口对。大致流程如下： RTP协议从上层接收流媒体信息码流，封装成RTP数据包； RTCP从上层接收控制信息，封装成RTCP控制包。 RTP将RTP 数据包发往UDP端口对中偶数端口；RTCP将RTCP控制包发往UDP端口对中的接收端口。 五、会话描述协议 SDP详解什么是SDP？SDP 全称 Session Description Protocal，直译就是通用会话描述协议。 光看直面意思可能不太好理解，其实就是描述双方的会话信息，以及各端所具备的能力的通用协议。 在WebRTC中SDP说描述信息主要有：(1)各端所支持音视频编解码器。(2)编解码所设定的参数。(3)所使用的的传输协议。(4) ICE连接候选项等。 标准 SDP 规范SDP并不是WebRTC独有的，关于标准的SDP规范可以参考。RFC4556规范其规范主要包括SDP描述格式 和 SDP结构，而 SDP结构由会话描述和媒体信息描述两个部分组成。 SDP 的格式SDP 是由多个 &lt;type&gt;=&lt;value&gt;这样的表达式组成的。 1234v&#x3D;0o&#x3D;- 7017624586836067756 2 IN IP4 127.0.0.1s&#x3D;-t&#x3D;0 0... type只能为一个字符，代表属性。 value为结构化文本，UTF8编码，代表属性值。 =两边不能有空格。 SDP 的结构SDP的结构有一个会话描述和多个媒体信息描述组成。 1.会话描述常见属性 v= sdp协议版本 v=0 o= 会话发起者描述 o=&lt;username&gt; &lt;sess-id&gt; &lt;sess-version&gt; &lt;nettype&gt; &lt;addrtype&gt; &lt;address&gt; username：用户名sess-id：会话id，在整个会话中是唯一的，建议使用 NTP 时间戳。sess-version：会话版本，每次会话数据修改后，该版本值会递增。nettype：网络类型，一般为“IN”。addrtype：地址类型，一般为 IP4。address：IP 地址。 s= 会话名 s=&lt;session name&gt; 不关注时可为 - t= 会话时间 会话开始时间、结束时间。t= 均为 NTP 时间，单位是秒；均为0时表示持久会话 c= 连接数据 c=&lt;nettype&gt; &lt;addrtype&gt; &lt;connection-address&gt;nettype：网路类型addrtype：地址类型connection-address：连接地址 2.媒体描述常见属性 m= 媒体描述 m=&lt;media&gt; &lt;port&gt; &lt;transport&gt; &lt;fmt-list&gt;media：媒体类型port：端口号transport： 传输协议RTP/AVP（RTP/SAVP）或UDP。fmt-list：媒体格式，表述RTP的数据负载类型(Payload Type)的列表，可以包含多个。RTP类型参考：RTP Payload a= 附加描述 有以下两种格式：a=a=: 在m=之前，为会话附加描述；在m=之后，为媒体附加描述。其中可以关注rtpmap和fmtp： rtpmap RTP 参数映射表 a=rtpmap:&lt;playload-type&gt; &lt;encoding-name&gt;/&lt;sample-rate&gt;/&lt;encodingparameters可选&gt; playload-type：数据负载类型。encoding-name： 编码名称。sample-rate：采样率。encodingparameters：编码参数。 fmtp 格式参数 `a=fmtp: playload-type：数据负载类型。specific-parameters：编码参数。 WebRTC的SDP在webrtc中的sdp相对于标准sdp规范中有点不一样，它对于sdp划分了更多部分： WebRTC 按功能将 SDP 划分成了五部分，即会话元数据、网络描述、流描述、安全描述以及服务质量描述。WebRTC SDP 中的会话元数据（Session Metadata）其实就是 SDP 标准规范中的会话层描述；流描述、网络描述与 SDP 标准规范中的媒体层描述是一致的；而安全描述与服务质量描述都是新增的一些属性描述。 六、WebRTC媒体协商Offer &amp; Answer什么是媒体协商简而言之，在没有建立webrtc连接前，首先让本地和远端确认彼此共同支持的媒体能力。如：音视频编解码器、使用的传输协议、IP端口和传输速率等等。而这些信息需要通过前文所说的SDP来互换。 媒体协商流程这里以“发起端”和“应答端”来进行流程说明。 简单概括就是：发起端和应答端通过creatOffer和createAnswer创建offer/answer SDP，然后通过信令服务互换，最后调用setLocalDescription/setRemoteDescription进行设置本地和远端的SDP以完成协商。 在双方都创建RTCPeerConnection之后, 它们就可以开始进行媒体协商了。 首先发起端调用RTCPeerConnection.createOffer创建一个offer，并调用setLocalDescription设置本地的SDP。 然后通过信令服务器 将含有 SDP的offer设置给应答端。 应答端拿到此offer以后调用setRemoteDescription将此SDP信息保存。 应答端调用RTCPeerConnection.createAnswer创建一个answer，并调用setLocalDescription设置本地的SDP。 通过信令服务器将含有SDP的answer发送给发起端。 发起端调用setRemoteDescription将此SDP信息保存。 媒体协商的代码实现这里主要拆分为四步： 发起端创建 Offer &amp; 收到 Offer。 123456//localconst pc_local = new RTCPeerConnection(otps1);pc_local.createOffer((offer)=&gt;&#123; pc_local.setLocalDescription(offer); singalChannel.send(offer)&#125;, handleError); 应答放收到offer 1234567//remoteconst pc_remote = new RTCPeerConnection(otps2);signalChannel.on('message',(message)=&gt;&#123; if(message.type==='offer')&#123; pc_remote.setRemoteDescription(new RTCSessionDescription(message)) &#125;&#125;) 应答方创建 Answer 12345//remotepc_remote.createAnswer((answer)=&gt;&#123; pc_remote.setLocalDescription(answer); singalChannel.send(answer);&#125;, handleError ); 呼叫方收到 Answer 123456//localsignalChannel.on('message',(message)=&gt;&#123; if(message.type==='answer')&#123; pc_local.setRemoteDescription(new RTCSessionDescription(message)) &#125;&#125;) 以上整个协商才算完成，之后 WebRTC 才会在浏览器底层会收集 Candidate，并进行连通性检测，最后进行ICE连接和媒体数据传输。 七、WebRTC的NAT打洞与连接：STUN/TUN/ICE什么是NAT或许你在前一段时间听闻IPv4地址枯竭的新闻，IPv4地址只有32位长，理论最多42.9亿条。大概在94年时候，提出了IP网络地址转换（NAT）RFC规范，作为一个临时方案来解决IPv4地址枯竭的问题。 这个方案就是把IP地址重用，在边缘网络引入NAT设备，由它来负责维护本地服务IP和端口的映射到公网IP和端口。NAT内部的本地IP地址空间可以被许多不同的子网络重用，从而解决地址耗尽的问题。 可是，后面临时方案很快成为了最终方案，成为了互联网基础设施的组成部分。它不仅用来解决IP地址枯竭的问题，你能发现路由器、防火墙、代理设备都具备NAT功能。 NAT类型关于NAT被人们研究总结过，大体两种：锥型和对称型。而锥型又可分细分为三种。 所以归纳起来，总共四种类型：完全锥型、IP限制锥型、端口限制锥型 和 对称型。 完全锥型 当内网host与外网机器通讯，就会在NAT上打洞，这个过程就是指在NAT建立内外网映射表，这个表上记录内外网IP端口映射关系。外网机器与P地址p端口的通讯，都会在NAT上转发到对应的内网地址与端口，从而实现和内网host机器通讯。 IP限制型 IP限制锥型更加严格，在完全锥形基础上，只允许Host访问过的IP通过打洞。（映射表多记录了被访问外网的IP地址）如图，A和C等其它外网主机想通过B机的打洞IP端口，也是无法和Host通讯。 端口限制锥型 端口限制锥型比IP限制锥型更加严格，IP限制锥型不限制端口。IP限制锥型情况下，同一外网主机的其它端口服务，都能和内网Host通讯。但是端口限制性情况下，只允许外网机器指定的端口服务通过打洞。（映射表多记录了被访问的外网的IP地址和端口） 对称型 对称型，是最严格的NAT类型，它一样有IP和端口限制（这点和端口限制锥型一致）。它最大不同在于，每次连接都会在NAT上新开IP地址或者新开IP地址和端口与外网通讯。也就是说每一次NAT的打洞都不相同。基本上对称型NAT是无法穿越的。 NAT类型检测下图是在STUN（Session Traversal Utilities for NAT 翻译过来会话穿越NAT实现，可以理解为NAT穿越的算法协议。）中，对于NAT类型检测的流程图。当走到红色结束时：表明穿越失败，无法UDP通讯。当走到绿色（公网地址）或黄色时（锥型NAT）才可能进行通讯。 整个流程大体上发起了5次检测test1 主机向服务器IP端口发请求，服务器通过同样IP端口返回，收到返回？yes：next。no：udp不通。 是否为同一个地址？yes：没有NAT=&gt;走test2。 no：在NAT后=&gt;走test3。 test2 判断返回的主机外网IP地址是否与主机自身IP地址是否一样? yes：是公网地址；no：存在对称型防火墙。 test3 在 NAT背后，主机向服务器发请求，服务器通过另一个网卡IP和不同端口返回，是否收到。 yes：完全锥型；no：限制锥形=&gt;test4。 test4 主机向另一台服务器发请求，对比服务器1和2的上获取的本机映射的公网IP端口是否一致？ yes：非对称型（至少IP不限制）；no：对称型=&gt;test5。 test5 主机向test4的服务器再发一次请求，该服务器用相同IP但不同的端口返回？ yes：IP限制锥型；no：端口限制锥型 以上关于NAT，及其打洞和穿越原理的介绍。实际穿越比这个更多细节且更加复杂，有相应的RFC规范RFC5389，但是整个耗时还是相当快的。 STUN/TURN 协议前文提到，客户主机不可避免的在防火墙或NAT之后。在UDP传输时，一般只会带上 NAT 的 host。如果没有目标机器的entry是不会转发到目标机器。在client-server情形下没有问题。如果是 peer-to-peer 情形下则无法传输。因此我们需要借助STUN/TURN方式进行NAT穿透。 WebRTC采用了ICE（Interactive Connectivity Establishment）建立端到端的数据通道。说道ICE，就不得不提到它的两个工具协议：STUN(Session Traversal Utilities for NAT)和TURN（Travelsal Using Relays around NAT）协议。 STUN标准规范定义STUN，首先在RFC3489中定义，作为一个完整的NAT穿透解决方案，英文全称是Simple Traversal of UDP Through NATs，即简单的用UDP穿透NAT。 在RFC5389修订中把STUN协议定位于为穿透NAT提供工具，而不是一个完整的解决方案，英文全称是Session Traversal Utilities for NAT，即NAT会话穿透效用。 STUN用途 会话参与双方获取对方的IP地址和端口 检测两端点间的连接性 维持NAT绑定 STUN服务的简易过程内网主机需要借助STUN 服务器，遵循STUN协议机制，便能得到本机 NAT 映射后的外网的 IP 和端口，以下是简易过程。 首先在搭建一个 STUN 服务器，现在比较流行的 STUN 服务器是 CoTURN。 内网主机发送一个 binding request 的 STUN 消息到 STUN 服务器。 STUN 服务器收到 binding request后，会将请求的 IP 地址和端口填充到 binding response 消息中，再原路将该消息返回给内网主机。 收到 binding response 的内网主机就会解析 binding response 消息了，并可以从中得到自己的外网 IP 和端口。 TURN标准规范定义TURN，在RFC5766中定义，英文全称Traversal Using Relays around NAT（TURN）：Relay Extensions to Session Traversal Utilities for NAT（STUN），即使用中继穿透NAT：STUN的中继扩展。简单的说，TURN是通过两方通讯的“中间人”方式实现穿透。注意：这里Relay 或 TURN 是同一个概念都是值中继型传输。 用途当STUN服务检测，发现直接以peer-to-peer的形式连接时，就走TURN方式，使用中间网点提供的中继连接服务。TURN协议就是用来允许主机控制中继的操作并且使用中继与对端交换数据。TURN与其它中继控制协议不同的是它能够允许一个客户端使用一个中继地址与多个对端连接。 ICE连接机制收集ICE Candidate（候选项）WebRTC两端要进行连接时，每一端都会提供多个候选者，比如一端有两块网卡，那么每块网卡的不同端口都对应一个候选者。 ICE Candidate 主要分为以下三种类型： host 类型：即本机内网的 IP 和端口 srflx 类型：即本机 NAT 映射后的外网的 IP 和端口 relay 类型：即中继服务器的 IP 和端口 一般由以下字段组成 1234567IP: xxx.xxx.xxx.xxx \\\\IP地址port: number \\\\端口type: host/srflx/relay \\\\类型priority: number \\\\优先级protocol: UDP/TCP \\\\传输协议usernameFragment: string \\\\访问服务的用户名... ICE 连接流程连通性检测当收集完 ICE Candidate后，双方通过信令通道交换，并拿到彼此的ICE candidate之后，WebRTC 就开始按优先级顺序进行连通性检测了。 一般情况下：host 类型的候选者优先级是最高的, srflx类型次之, 最后是relay类型。 这个阶段对于对等项发来的通过身份验证的任何STUN连接request，ICE代理都会生成STUN response。候选项依照之前的排序按次序进行检查，当收到对方的响应时，检查视为成功，而如果检查超时后仍没有收到响应时，则该候选对失败。 连通性检测的原则简单归纳: 以一定的优先级将候选地址对进行排序 以该优先级顺序发送checks请求 从其它终端接收到checks的确认信息 选定候选项 在WebRTC中 P2PTransportChannel会维护连接状态表，并排序表中记录（SortConnectionsAndUpdateState）。 排序指的是计算每条记录的连接“成本”，把成本最低的排在第一位。如何计算成本？则涉及到很多因素，比如发出Stun请求到收到应答耗时，用时越少的“成本”自然会低些。 当一端有视频Rtp数据要发送时，会检查状态表的第一条记录，如果判断出它的状态是发送就绪，就会用此Connection进行发送。否则直接放弃这个发送任务。 ICE 长连接和重启为了确保NAT映射和过滤规则不在音视频通话过程中超时，ICE会不断对使用中的候选项对（通道）进行连接检查，每15s发送一次，这样是为了保证在音视频流暂停等情况下没有发送数据流时，仍然有数据包持续发送。 当ICE代理检测到正在使用的传输地址发生更改或连接时，会触发重新启动ICE事件，也就是会重新回到收集ICE candidate及其之后的流程。 总结：ICE 就是包括 STUN、TURN 协议的一套框架，用于找到一条可用且最优传输数据通道连接。 八、WebRTC核心API之RTCPeerConnection什么是RTCPeerConnectionRTCPeerConnection API是Peer to Peer连接的核心，它是WebRTC核心组件，用于处理对等体之间流数据的稳定和有效通信。可以这么去理解：每一个RTCPeerConnection都是一个客户端。 我们通过RTCPeerConnection 这个API，即可简单创建和操控RTC连接对象。并且无需过分关注细节，即可获得浏览器的底层能力如：编解码、丢包隐藏、回声消除、带宽适应性、动态抖动缓冲、自动增益、降噪抑制等 在音视频通话中，每一方只需要有一个 RTCPeerConnection 对象，用它来接收或发送音视频数据。 RTCPeerConnection API 适配各种浏览器 在 WebRTC 1.0 规范没有发布之前，各大浏览器厂商都早已移植了WebRTC，但是最终暴露给开发者的接口各不相同。 在 WebRTC 1.0 规范发布之后，情况有所好转，但对于诸多存量的老版本浏览器而言，开发者兼容起来仍然很麻烦。因此Google开发了 adapter.js 来做适配，直接引入就能用标准的API来开发WebRTC。 仓库地址：https://github.com/webrtcHacks/adapter 1&lt;script src=\"https://webrtc.github.io/adapter/adapter-latest.js&gt; &lt;/script&gt; RTCPeerConnection 工作流程在 WebRTC 端与端之间建立连接，主要分为四个步骤 每个端创建一个 RTCPeerConnection 对象，上行就给 RTCPeerConnection 对象添加一个本地流。 获取本地SDP（会话描述协议），并与对端进行交换，并在设置进RTCPeerConnection中。 获得网络信息，即 Candidate（IP 地址和端口），并与远端进行交换并设置。 在ICE连接成功后，下行收看则通过监听RTCPeerConnection的流添加事件来获取远端流。 RTCPeerConnection主要方法、属性及事件Constructor123456789101112131415161718const configuration=&#123; bundlePolicy, // 协商策略，balanced、max-compat、max-bundle,基本上是选择max-bundle，主要是防止另一个客户端属于策略不可协商型。 certificates,//RTCCertificate对象数组，在认证的连接中使用. iceCandidatePoolSize,//16位整数值，指定预取的ICE候选者池的大小。默认值为0（表示不会进行任何候选预取） iceServers,//RTCIceServer对象数组，每个对象描述ICE代理服务器，通常是STUN或TURN服务器。 iceTransportPolicy,//ICE传输原则: //ALL(全部收集),NOHOST(不收集host类的策略信息),RELAY(只使用服务器的策略信息，简言之就是不通过P2P，只走服务端流量),NONE(不收集策略信息)。 peerIdentity,//用于指定的目标对等身份RTCPeerConnection。 rtcpMuxPolicy,////实时传输控制协议多路策略，negotiate、require，第一个是获取实时传输控制协议策略和实时传输协议策略，第二个只获取实时传输协议策略，如果另一个客户端不支持实时传输控制协议，那么协商就会失败。&#125; const pc = new RTCPeerConnection(configuration); Method1. 添加本地流 addStream 1234567const pc = new RTCPeerConnction();navigator.getUserMedia(&#123;video: true&#125;, function(stream) &#123; pc.onaddstream(&#123;stream: stream&#125;); // Adding a local stream won't trigger the onaddstream callback pc.addStream(stream);&#125;) 2. 媒体协商之创建SDP createOffer/createAnswer 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/*offer*/pc.createOffer().then(function(offer) &#123; return pc.setLocalDescription(answer);&#125;).then(function() &#123; // Send the offer to the remote peer through the signaling server.&#125;)/*answer after set the remote peer offer*/pc.createAnswer().then(function(answer) &#123; return pc.setLocalDescription(answer);&#125;).then(function() &#123; // Send the answer to the remote peer through the signaling server.&#125;)``` **3. 媒体协商之设置SDP setLocalDescription/setRemoteDescription**```javascript//设置本地创建的sdppc.setLocalDescription(new RTCSessionDescription(offer/answer).then(()=&gt;&#123; //success&#125;).catch(error=&gt;&#123;&#125;)//设置远端创建的sdppc.setReomteDescription(new RTCSessionDescription(offer/answer).then(()=&gt;&#123; //success&#125;).catch(error=&gt;&#123;&#125;)``` **6. 添加Ice候选项 addIceCandidate**```javascript// A typical value of ice here might look something like this:// &#123;candidate: \"candidate:0 1 UDP 2122154243 192.168.1.9 53421 typ host\", sdpMid: \"0\", ...&#125;pc.addIceCandidate(message.ice).catch(e =&gt; &#123; console.log(\"Failure during addIceCandidate(): \" + e.name);&#125;); Event Handlers1. 监听远端添加流/移除流 onaddStream onremovestream 12345pc.onaddstream = function(event) &#123; document.getElementById(\"received_video\").srcObject = event.stream;&#125;;pc.onremovestream = function(ev) &#123; alert(\"onremovestream event detected!\"); &#125;; 2. 监听获取ICE候选项 onconnectionstatechange 1234567pc.onicecandidate = function(event) &#123; if (event.candidate) &#123; // Send the candidate to the remote peer &#125; else &#123; // All ICE candidates have been sent &#125;&#125; 3. 监听RTCPeerConnection状态变化 onconnectionstatechange 1234567891011121314pc.onconnectionstatechange = function(event) &#123; switch(pc.connectionState) &#123; case \"connected\": // The connection has become fully connected break; case \"disconnected\": case \"failed\": // One or more transports has terminated unexpectedly or in an error break; case \"closed\": // The connection has been closed break; &#125;&#125; 4. 监听’ICE收集状态’和’ICE连接状态’的变化 1234567891011121314151617181920212223pc.onicegatheringstatechange = function() &#123; let label = \"Unknown\"; switch(pc.iceGatheringState) &#123; case \"new\": case \"complete\": label = \"Idle\"; break; case \"gathering\": label = \"Determining route\"; break; &#125; document.getElementById(\"iceStatus\").innerHTML = label;&#125;pc.oniceconnectionstatechange = function(event) &#123; if (pc.iceConnectionState === \"failed\" || pc.iceConnectionState === \"disconnected\" || pc.iceConnectionState === \"closed\") &#123; // Handle the failure &#125;&#125;; Properties1. connectionState：RTCPeerConnection连接状态 “new” “connecting” “connected” “disconnected” “failed” “closed” 2. iceConnectionState：ICE连接状态 “new” “checking” “connected” “completed” unified-plan not have this state “disconnected” “failed” “closed” 3. iceGathergingState：ICE收集状态 “new” “gathering” “completed” 4. currentLocalDescripiton 、currentRemoteDescription：当前设置的本地/远端SDP 理解RTCPeerConnection 类，如果从使用的角度看，RTCPeerConnection 是一个接口类；如果从功能的角度看，它又是一个功能聚合类。 九、WebRTC的安全机制DLTS协议我们都知道 HTTPS 的底层最初是使用 SSL协议（Secure Sockets Layer）进行数据加密。IETF 对 SSL 3.0进行标准化，并且增加新功能，才有了TLS 1.0（Transport Layer Security，安全传输层协议）。 TLS 协议由 TLS 记录协议和 TLS 握手协议组成： TLS 记录协议，用于数据的加密、数据完整性检测等； TLS 握手协议，主要用于密钥的交换与身份的确认。 TLS底层基于TCP协议，但WebRTC音视频数据传输基于UDP协议，因此无法直接使用TLS。因此有了基于UDP 协议之上的简化版本的 TLS，那就是DTLS。 在 WebRTC 中为了有效地保护音视频数据，使用 DTLS 协议交换公钥证书，并确认使用的密码算法，这个过程在 DTLS 协议中称为握手协议。 首先 DTLS 协议采用 C/S 模式进行通信，其中发起请求的一端为客户端，接收请求的为服务端。 客户端向服务端发送 ClientHello 消息，服务端收到请求后，回 ServerHello 消息，并将自己的证书发送给客户端，同时请求客户端证书。 客户端收到证书后，将自己的证书发给服务端，并让服务端确认加密算法。 服务端确认加密算法后，发送 Finished 消息，至此握手结束。 DTLS 握手结束之后，通信双方就可以开始相互发送音视频数据了。 WebRTC身份辨别校验通过 DTLS 协议解决了交换公钥时可能被窃取的问题，但在多人音视频场景中，还需要解决辨别身份的问题。 在WebRTC中，需要通过信令服务交换SDP，其中就包含身份验证的所需要信息： 123a&#x3D;ice-ufrag:khLSa&#x3D;ice-pwd:cxLzteJaJBou3DspNaPsJhlQa&#x3D;fingerprint:sha-256 FA:14:42:3B:C7:97:1B:E8:AE:0C2:71:03:05:05:16:8F:B9:C7:98:E9:60:43:4B:5B:2C:28:EE:5C:8F3... ice-ufrag 和 ice-pwd 相当于用户名和密码。fingerprint 它是存放公钥证书的指纹，在通过 ice-ufrag 和 ice-pwd 验证用户的合法性之外，还通过它对收到的证书做验证。 SRTP/SRTCP由于RTP/RTCP协议并没有对传输的负载数据 进行任何保护，也就是说可能会在传输工程中被抓包工具抓取，所以仍然存在不安全因素。因此WebRTC使用了SRTP/SRTCP 协议 ，即安全的 RTP/RTCP 协议。 WebRTC使用有名的 libsrtp 库将原来的 RTP/RTCP 协议数据转换成 SRTP/SRTCP 协议数据，主要分为以下几步： 初始化libsrtp。 创建Session (指定会话过程中的策略，如：算法进行内容的完整性检测，解码时的公钥等等)。 对 RTP包 加密，传输为 SRTP包。 对 SRTP包 解密，得到 RTP包。 最后是释放资源。 十、WebRTC的质量数据监控与分析WebRTC流（音频，视频或数据）在传输过程中，会因为网络不稳定的而丢失。在实际的复杂的网络环境中，这种不稳定往往时刻影响WebRTC的音视频质量。因此WebRTC在底层已经实现对网络传输质量的监控和评估能力（例如：实时码率、丢包率，视频帧率等），并以此去实时动态调节和优化传输策略。 浏览器webrtc-internal监控数据当你在浏览器使用WebRTC的时候，并且在地址栏输入：chrome://webrtc-internals，就能看到webrtc所监控的质量数据。 打开后，在视图最顶端可以看到默认用 标准API 展示统计信息。（也可以选用传统老式API展示）。 选取对应页面指定的RTCPeerConnection。 可以看到：诸如“Stats Graphs”各项实时质量图表，诸如”RTCMediaStream_Track_receiver”代表接收的音视频轨信息。 音频轨监控图表包含：jitterBuffer，audioLevel，以及音频采样信息等。 视频轨监控图表包含：帧的宽高信息，接收帧数，解码帧数，丢弃帧数等。 getStat APIgetStats是WebRTC重要的API之一，开发者通过它获取WebRTC实时运行的状态信息，包括网络数据收发状态、P2P客户端媒体数据采集和渲染状态等。通过这些能力，我们能无需关注浏览器底层实现的情况下，轻易监控到WebRTC运行状态，并及时定位排除错误。 getStats 返回数据getStats能够采集以下几大类的度量核心数据： 发送端采集统计：对应于媒体数据的生，包括帧率，帧大小，媒体数据源的时钟频率，编解码器名称，等等。 发送端RTP统计：对应于媒体数据的发送，包括发送数据包数，发送字节数，往返时间RTT等等。 接收端RTP统计：对应于媒体数据的接收，包括接收数据包数，接收字节数，丢弃数据包数，丢失数据包数，网络抖动jitter，等等。 接收端渲染统计：对应于媒体数据的渲染，包括丢弃帧数，丢失帧数，渲染帧数，渲染延迟等等。 getStat的JS调用和RTCStatsReport调用方法： 1const promise = rtcPeerConnection.getStats(selector) selector : 选择要收集的媒体轨，默认为null，则收集整个RTCPeerConnection。 getStats 调用后，会异步返回 RTCStatsReport 数据对象的数组。每个RTCStatsReport包含三个值： id：对象的唯一标识。 timestamp：时间戳，RTCStatsReport生成时间。 type：RTCStats的类型。具体可参考：RTCStatsType 遍历每个RTCStatsReport： 12345678910rtcPeerConnection.getStats().then( reports =&gt; &#123; //遍历每个 RTCStatsReport reports.forEach( report =&gt; &#123; console.log(report); &#125;); &#125;).catch( err=&gt;&#123; console.error(err); &#125;);); 这里可以关注以下 type为 inbound-rtp 和 outbound-rtp，分别为接收（下行）rtp流相关数据，和发送（上行）rtp流相关数据。 在下图type为inbound-rtp的report中，可以知道 rtp video stream的收丢包情况、解帧情况等，并可以以此作为监控上报的数据。 在 WebRTC 1.0 规范中，一共定义了 17 种 RTCStats 类型的子类，它们值对应含义： 每种类型的 RTCStatsReport 中包含许多数据属性，每种属性代表的含义需要基础音视频基础才能理解。如果想知道属性的具体含义或者计算方式，可以参考webrtc规范给的解释：https://w3c.github.io/webrtc-stats/#rtctatstype-*","categories":[],"tags":[]}]}